---
title: "IST 707 Project"
author:
  - Data Degenerates
  - Daniel Caley
  - Michael Johnson
date: "9/4/2021"
output:
  word_document:
    toc: TRUE
---

\pagebreak
# Introduction

Recommender systems provide the backbone of systems designed to both enhance user workflow and to increase viewers for the overall platform. Using the Netflix data, successful models relying on various techniques help in building out a recommendation network that includes predicting the IMDb score and parental ratings for suggested viewing. When used in conjunction with Association Rule Mining techniques for genre, the recommender system will have robust capabilities that include a variety of models for people viewing everything from G to R ratings.

Four primary techniques make up the base for conducting analysis, exploring the data set, and building a predictive model that centers on the IMDb score and a parental rating classifier. Clustering and Classification on the description illuminate the parental rating and serve as a basis for comparing different descriptions. Association Rule Mining will help suggest what genre of movies or TV shows a viewer should watch next based on their most recently viewed. Identifying predictive solutions using Movie Length, Genre, Parental Rating, and TV Show or Movie as part of a Support Vector Machine (SVM), KNN, Random Forest, or Decision Tree model.

```{r setup, include=FALSE}

library(tidyverse)
library(dplyr)
library(data.table)
library(tm)
library(cluster)
library(ggplot2)
library(ggfortify)
library(plyr)

# Needed to introduce tuning parameters for machine learning
library(caret) # Machine Learning
library(caretEnsemble) # To ensemble predictions
library(questionr)# Required run Naive in caret
library(klaR) # Required for Naive Bayes in caret
library(e1071) # Required for Naive Bayes faster

# Needed for Decision tree model and plot
library(rpart) # decision tree
library(rpart.plot) # used for model plot
library(rattle) # used for model plot

library(arules) # used for association rule
library(arulesViz) # used for association rule

```

\pagebreak

# About the Data

The Netflix data set came from Satpreet Makhija (2021) on Kaggle, and it contains all the movies and TV shows from Netflix in 2001. The overall data is contained within the following columns: Description, Director, Genre, Cast, Rating, Duration and IMDb Score. Discretization of the parental ratings included combining ratings that aligned with certain age groups, like TV-MA and R. Although these have varying definitions for who should and should not be watching, the suggested age groups were close enough to combine together with a reasonable assumption that parents would not balk at the differences between G versus Y-7 or PG-13 versus TV-14.

### Cleaning Data

Filtering the original data included removing some columns and limiting the data based on location and type. As a United States-focused recommender system, the data includes all production studios that shot scenes specifically within the United States at some point in the process. Additionally, the data contains no blank values wherever data was missing for director and cast records. Having complete data for those variables could be an important part of future modifications to the recommender system that rely on using specific directors or cast members. Additionally, the data only contains movies as TV shows have varying data for the cast, directors, and other key variables that went into predicting IMDb score like duration.

Discretization and cleaning of the data included the following focus areas:

-   Removing "/10" from the IMDb score column.
-   Removing "min" from the duration column.
-   Removing the Date Added column.
-   Putting the length of movie in 30 minute bins.
-   Creating an International flag field for production country that sets a value of 0 for movies that only include the United states.
-   Cleaning up the rating by combining the TV ratings and movie ratings in the following format:
      -   TV-Y transformed to G.
      -   TV-Y7 transformed to PG.
      -   TV-14 transformed to PG13.
      -   TV-MA transformed to R.

```{r include = FALSE}

MyNetflix <- read_csv("data/netflixData.csv")
CleanFlix <- MyNetflix %>% filter(
                                  `Production Country` %like% "United States" &
                                  `Content Type` == "Movie" &
                                  Director != "" &
                                  Cast != ""
                                  ) %>% 
                           mutate(
                                  `Imdb Score` = as.numeric(str_remove(`Imdb Score`,"/10")),
                                  Duration = as.numeric((str_remove(Duration, " min"))),
                                  Duration_bins = as.ordered(ceiling(Duration / 30) * 30),
                                  internation_flag = str_count(`Production Country`) != 13
                           ) %>% dplyr::select(-`Date Added`)
 

## Cleaning the Ratings
CleanFlix <- CleanFlix %>% dplyr::mutate(
                                  Rating = str_remove(Rating,"TV-"),
                                  Rating = case_when(Rating == "Y" ~ "G",
                                                     Rating == "Y7" ~ "PG",
                                                     Rating == "14" ~ "PG-13",
                                                     Rating == "MA" ~ "R",
                                                     TRUE ~ Rating)
                                  ) %>% filter(!is.na(`Imdb Score`))

```

\pagebreak

The following output is the structure of the data, including variable types and field names:

```{r}

str(CleanFlix)

```

# Exploratory Data Analysis

```{r echo = FALSE, message=FALSE, warning=FALSE}


CleanFlix %>% ggplot() +
                aes(x = Rating) +
                geom_bar(fill = "Salmon", color = "Black") +
                theme_minimal() +
                ylab("Count")


print(paste0("Total Number of R Movies: ", sum(CleanFlix$Rating == "R")))
print(paste0("Total Number of PG-13 Movies: ", sum(CleanFlix$Rating == "PG-13")))
print(paste0("Total Number of PG Movies: ", sum(CleanFlix$Rating == "PG")))
print(paste0("Total Number of G Movies: ", sum(CleanFlix$Rating == "G")))

```

Looking at the count of movies with each rating shows a disparaging difference. Movies that fall within the “R” category represent over 57 percent of all movies and could skew results in favor of that rating. PG-13 movies make up just over 23 percent of the total with PG lagging behind at just over 14 percent and G at just over 4 percent of the total movies.

``` {r echo = FALSE, message=FALSE, warning=FALSE}
ggplot(CleanFlix) +
  aes(x = Duration) +
  geom_histogram(bins = 30L, fill = "Salmon", color = "Black") +
  theme_minimal() +
  ylab("Count") +
  geom_vline(aes(xintercept=mean(Duration)), col="Blue")

print(paste0("Average Duration in Minutes: ", mean(CleanFlix$Duration)))

```

The duration of movies has an interesting shape close to a normal distribution. The average time of movies just over 90 minutes shows a goal for most lengths at about the hour and a half mark with some outliers that run over 200 minutes in length.

``` {r echo = FALSE, message=FALSE, warning=FALSE}
vlinedata <- ddply(CleanFlix, "Rating", summarize, DurationMean=mean(Duration))

ggplot(CleanFlix) +
  aes(x = Duration) +
  geom_histogram(bins = 30L, fill = "Salmon", color = "Black") +
  theme_minimal() +
  facet_wrap(vars(Rating)) +
  geom_vline(aes(xintercept=DurationMean),  vlinedata, col="Blue") +
  ylab("Count") +
  ggtitle("Duration Split by Rating")

print(paste0("Mean Duration for R Movies: ", mean(CleanFlix$Duration[CleanFlix$Rating=="R"])))
print(paste0("Mean Duration for PG-13 Movies: ", mean(CleanFlix$Duration[CleanFlix$Rating=="PG-13"])))
print(paste0("Mean Duration for PG Movies: ", mean(CleanFlix$Duration[CleanFlix$Rating=="PG"])))
print(paste0("Mean Duration for G Movies: ", mean(CleanFlix$Duration[CleanFlix$Rating=="G"])))

```

Taking a closer look at the duration by parental rating indicates PG-13 movies have the longest run time, followed closely by R and PG. Movies with a G parental rating tend to be on the shorter side of the overall count, possibly due to the attention span of the audience.

``` {r echo = FALSE, message=FALSE, warning=FALSE}
ggplot(CleanFlix) +
  aes(x = `Imdb Score`) +
  geom_histogram(bins = 30L, fill = "Salmon", color = "Black") +
  theme_minimal() +
  ylab("Count") +
  ggtitle("IMDb Score") +
  geom_vline(aes(xintercept=mean(`Imdb Score`)), col="Blue")

print(paste0("Average IMDb Score: ", mean(CleanFlix$`Imdb Score`)))

```

The distribution of IMDb Scores has a negative skew with movies having more favorable scores with outliers approaching a score of 2 and 9 with a scale of 0 to 10. Based on the average IMDb around 6.2, the scale of IMDb scores indicates an imbalance where an expectation for movie averages would fall around the 5 mark.

``` {r echo = FALSE, message=FALSE, warning=FALSE}
vlinedata <- ddply(CleanFlix, "Rating", summarize, ScoreMean=mean(`Imdb Score`))

ggplot(CleanFlix) +
  aes(x = `Imdb Score`) +
  geom_histogram(bins = 30L, fill = "Salmon", color = "Black") +
  theme_minimal() +
  facet_wrap(vars(Rating)) +
  geom_vline(aes(xintercept=ScoreMean),  vlinedata, col="Blue") +
  ylab("Count") +
  ggtitle("IMDb Score Split by Rating")

print(paste0("Mean IMDb Score for R Movies: ", mean(CleanFlix$`Imdb Score`[CleanFlix$Rating=="R"])))
print(paste0("Mean IMDb Score for PG-13 Movies: ", mean(CleanFlix$`Imdb Score`[CleanFlix$Rating=="PG-13"])))
print(paste0("Mean IMDb Score for PG Movies: ", mean(CleanFlix$`Imdb Score`[CleanFlix$Rating=="PG"])))
print(paste0("Mean IMDb Score for G Movies: ", mean(CleanFlix$`Imdb Score`[CleanFlix$Rating=="G"])))

```

The first indication of possibly favoring R movies due to the sheer number within the data shows when comparing the IMDb scores between ratings. R movies average an IMDb score of 6.27, the highest out of the other movie ratings.

Alongside with looking at the base counts, the description of each of the movies could help with identifying the parental rating for future movies entering the recommender system. The first step includes pulling out the description for each movie and vectorizing the words with normalizing the frequencies between each movie.

\pagebreak
# Clustering
```{r include = FALSE}

NetflixCluster <- CleanFlix %>% dplyr::select(Description)

row.names(NetflixCluster) <- CleanFlix$`Show Id`
myCorpus <- Corpus(VectorSource(NetflixCluster$Description))
STOPS <- stopwords('english')
MyStopwords <- c()
RemoveSpecialChars <- function(x) gsub("[^a-zA-Z0-9 ]","",x)

myCorpus <- tm_map(myCorpus, RemoveSpecialChars)
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
myCorpus <- tm_map(myCorpus, tolower)
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpus <- tm_map(myCorpus, removeWords, MyStopwords)

dtm <- DocumentTermMatrix(myCorpus)
Netflix_DTM <- as.matrix(dtm)

```

``` {r include = FALSE}
# Vectorization

# Look at word frequencies
WordFreq <- colSums(as.matrix(Netflix_DTM))
(head(WordFreq, 10))
(length(WordFreq))
ord <- order(desc(WordFreq))

# Looking at the most common and least common words
MostCommon <- WordFreq[head(ord, 10)]
LeastCommon <- WordFreq[tail(ord, 50)]
```

``` {r echo = FALSE, message=FALSE, warning=FALSE}

# Creating a barplot for the top 20 words
barplot(head(sort(WordFreq, decreasing = TRUE),10), main = "Ten Most Common Words")
#barplot(head(sort(WordFreq, decreasing = FALSE),20), main = "Twenty Least Common Words")

## Row Sums per Fed Papers
Row_Sum_Per_doc <- rowSums(Netflix_DTM)
print(paste0("Avergae Number of Words Per Description: ", floor(mean(Row_Sum_Per_doc))))

```

After removing all common words, stopwords, punctuation, numbers, and whitespace, the average number of words for each description comes out to 14 total words. This might limit the success of using clustering to identify trends in the data that could help with sorting movies between various categories. The goal is correctly identifying parental rating, but clustering will also show if there is anything within the data that could sort the movies. The ten most common words highlight the sentiment for overall movies on Netflix with “new” taking the top spot, followed closely by “life”. 

```{r include = FALSE}

# Create a normalized version
NetflixNormalized <- as.matrix(Netflix_DTM)
NetflixNormalized_N1 <- apply(NetflixNormalized, 1, function(i) round(i/sum(i),3))
Netflix_Matrix_Norm <- t(NetflixNormalized_N1)
Netflix_Matrix_Norm = as.matrix(Netflix_DTM)
Netflix_Norm_DF <- as.data.frame(as.matrix(Netflix_Matrix_Norm))
row.names(Netflix_Norm_DF) <- CleanFlix$`Show Id`

Netflix_Norm_DF_Freq <- Netflix_Norm_DF / length(colSums(as.matrix(Netflix_DTM)))


```

```{r include = FALSE}

# Set seed for fixed random seed
set.seed(73)

# Creating the number of clusters
cluster_loop <- c(2:20)

# Creating data frame to store cluster stats
Best_Cluster <- data.frame()

x <-  2
# Looping through the cluster number loop to find the best number of clusters
for (x in cluster_loop){

  # Run kmeans function with each cluster number
  Netflix_Norm_DF_Freq <- Netflix_Norm_DF / length(colSums(as.matrix(Netflix_DTM)))
  Clusters <- kmeans(Netflix_Norm_DF_Freq, x)

  # Set the Cluster as a variable in the data frame
  Netflix_Norm_DF_Freq$Clusters <- as.factor(Clusters$cluster)

  Cohesion_Clus <- Clusters$tot.withinss
  Separation_Clus <- Clusters$betweenss


  data_frame <- data.frame("Number" = x, "Cohesion" = Cohesion_Clus, "Separation" = Separation_Clus)
  Best_Cluster <- rbind(Best_Cluster, data_frame)

  # Convert all columns to numeric in order to plot using pcom
  #Netflix_Norm_DF_Freq <- sapply(Netflix_Norm_DF_Freq, as.numeric)

  # print(paste0("Completed Cluster: ", x))
}

```

The next step in the clustering process identifies the best number of clusters to use based on measured values of cohesion within clusters and separation between clusters. Looking for values with higher separation and lower cohesion will highlight the best number of clusters for the data if any sorted groups form. Looking at clusters between 2 and 20 should allow for the best distribution of clusters without worrying about having too many clusters for the data.

The following is the distribution of cohesion and separation for the top five clusters:

\pagebreak
## Best Cluster Results
```{r echo = FALSE}

# Finding best number of clusters
BestCluster <- data.frame(Best_Cluster$Number, Best_Cluster$Cohesion, Best_Cluster$Separation)
colnames(BestCluster) <- c("Number", "Cohesion", "Separation")
BestCluster$ScaleCohesion <- scale(BestCluster$Cohesion)
BestCluster$ScaleSeparation <- scale(BestCluster$Separation)
BestCluster$Combined <- (BestCluster$ScaleCohesion + BestCluster$ScaleSeparation)

BestCluster[head(order(-BestCluster$Combined), 5),]

```

The best number of clusters after scaling the cohesion and separation for each of the clusters tested and adding them together came out to 6 clusters.

The next step is running through the data with 6 clusters using the kmeans function on the normalized word frequencies.

```{r include = FALSE}
# Set seed for fixed random seed
set.seed(73)

# Run k-means with 6 clusters
x <- 6
Clusters <- kmeans(Netflix_Norm_DF_Freq, x)

# Filling data frame with movies
ClusterFlix <- CleanFlix

# Adding cluster to original data frame
ClusterFlix$Clusters <- as.factor(Clusters$cluster)

# Adding Cluster to the frequency data frame
Netflix_Norm_DF_Freq <- Netflix_Norm_DF / length(colSums(as.matrix(Netflix_DTM)))
Netflix_Norm_DF_Freq$Clusters <- as.factor(Clusters$cluster)

# Plots
Netflix_Norm_DF_Freq <- sapply(Netflix_Norm_DF_Freq, as.numeric)

```


```{r echo=FALSE, message=FALSE, warning=FALSE}

plot_title <- paste0("Cluster ",x)
 pcom <- prcomp(Netflix_Norm_DF_Freq, scale. = TRUE)
ggplot2::autoplot(pcom, data = Netflix_Norm_DF_Freq, colour = 'Clusters', frame = TRUE, frame.type = 'norm') +
   ggtitle(plot_title)

```

The graph of 6 clusters ends up looking pretty similar to the rest of the cluster amounts, even though it was the best in terms of cohesion and separation. The low frequency of words in the description with an average of 14 unique words per description highlights the limitations of relying on clustering to group this data.

```{r echo=FALSE, message=FALSE, warning=FALSE}

ggplot(data=ClusterFlix, aes(x=Rating, fill=Clusters))+
  geom_bar(stat="count") +
  labs(title = "Comparing Different Clusters") +
  theme(plot.title = element_text(hjust=0.5), text=element_text(size=15))

```

Looking at the bar chart next specifically for the goal of grouping parental rating shows a spread of parental values for all ratings. Ideally, the clustering would show each cluster falling within a specific rating.

```{r echo=FALSE, message=FALSE, warning=FALSE}
ggplot(data=ClusterFlix, aes(x=`Imdb Score`, fill=Clusters))+
  geom_bar(stat="count") +
  labs(title = "Comparing Different Clusters") +
  theme(plot.title = element_text(hjust=0.5), text=element_text(size=15))

```

As a point of interest, clustering also shows difficulty in identifying movies with all IMDb scores. Although the results do not indicate clustering as a viable method, forcing the goal of finding parental rating from description with classification may be possible.

\pagebreak
# Classification

```{r include = FALSE}

ClassFlix <- data.frame(Netflix_Norm_DF_Freq)
rownames(ClassFlix) <- CleanFlix$`Show Id`
ClassFlix$Rating <- CleanFlix$Rating
row_count <- nrow(ClassFlix)

ClassFlix <- ClassFlix %>% dplyr::mutate(
                                  random_seed = runif(row_count,1, 100) / 100,
                                  model_group = if_else(random_seed<= 0.50, "ClassTrain","ClassTest"),
                                  Train = model_group == "ClassTrain",
                                  Test = model_group == "ClassTest",
                                  )

```


Next steps is to break the data into a Test and Train set to be about 50% for both groups.

```{r echo=FALSE, message=FALSE, warning=FALSE}
sapply(ClassFlix[,6991:6992], sum)
barplot(sapply(ClassFlix[,6991:6992], sum) / row_count, main = "Test & Training IMDB")
```



```{r include = FALSE}
TrainClassFlix <- ClassFlix[ClassFlix$Train==TRUE,]
TrainClassFlix <- TrainClassFlix[ , -which(names(TrainClassFlix) %in% c("Train","Test", "random_seed", "model_group", "Clusters"))]

TestClassFlix <- ClassFlix[ClassFlix$Test==TRUE,]
TestClassFlix <- TestClassFlix[ , -which(names(TestClassFlix) %in% c("Train","Test", "random_seed", "model_group", "Clusters"))]

```

```{r include=FALSE}

# Train Tree Model 1
set.seed(73)
TreeFlix <- rpart(Rating ~ ., data = TrainClassFlix, method="class", control=rpart.control(cp=0, maxdepth = 5))
ClassPredicted <- predict(TreeFlix, TestClassFlix, type="class")
ClassFlixResults <- data.frame(Actual=TestClassFlix$Rating, TrainTreeModel1 = ClassPredicted)

```

```{r echo=FALSE, message=FALSE, warning=FALSE}

# Plot number of splits
rsq.rpart(TreeFlix)

```

The R-Square chart shows a difference in the relationship between Apparent and X Relative showing as the number of splits increases, the separation between the two also increases.

The X Relative Error decreases to the lowest point at one split and includes more error at increased split levels.

```{r echo=FALSE, warning=FALSE}

# Plot the decision tree
fancyRpartPlot(TreeFlix,palettes=c("Blues", "Reds"), main="Rating Decision Tree", sub = "")

```

Due to the low frequency of words throughout all the movies, the max depth set at 5 helps cut down the decision tree for use in classifying the movies based on parental rating. As the max depth increases, the accuracy of the model decreases.

Based on the chart, using the word “evil” is the first word to group within the entire decision tree. With only 2 percent coming off the total 100 percent, this decision chart shows the difficulties of not having a wide variety of movies between all ratings and a low word frequency for each description.

The following confusion matrix indicates the difficulties of using classification as a way to identify parental rating.

```{r echo=FALSE, warning=FALSE}
#confusion matrix to find correct and incorrect predictions
ClassTable <- table(Rating=ClassPredicted, true=TestClassFlix$Rating)
ClassTable

print(paste0("Correct Ratings: ", sum(diag(ClassTable))/sum(ClassTable)))
print(paste0("Incorrect Ratings: ", 1 - sum(diag(ClassTable))/sum(ClassTable)))

```

\pagebreak
# Exploratory Data Analysis on IMDb Scores

When building the model the target variable will be predicting IMDb score. Before jumping into the models, performing some exploratory analysis will help in identify different attributes and show the overall structure of the data.

- Boxplot Rating
  - Breaking IMDb score by rating shows that PG, PG-13, and R all have around the same average and are distributed similarly.
  - The tails and outliers are different for PG, PG-13, and R, but not too different.
  - When looking at the Rating score for G, the average and median is different than the remaining cohort of data.

- Boxplot Duration Bins
  - When looking at the boxplot, the duration bin between 1.5 and 2.0 hours appear to have somewhat normally distributed data.
  - The remaining duration bins either become smaller and/or are not as evenly distributed.

- Histogram International
  - Breaking the data out as International or domestic, there is quite more data in the domestic category.
  - Both have a sizable amount of data and are left skewed.
  - Domestic here means that the film is exclusively released in the United States and no other country.
  
- Overall
  - When breaking data into multiple dimensions, the information can be less actionable due to not having enough data.
  - The data here, as different dimensions are applied, shows that in most cases the data is not unevenly distributed.
  - As a point of caution, adding additional dimensions into the model will decrease the predictive capabilities.

```{r echo=FALSE, message=FALSE, warning=FALSE}



ggplot(CleanFlix) +
  aes(x = Rating, y = `Imdb Score`) +
  geom_boxplot(shape = "circle", fill = "lightblue", color = "black") +
  theme_minimal() +
  labs(
    title = "Boxplot Rating"
  )


ggplot(CleanFlix) +
  aes(x = Duration_bins, y = `Imdb Score`) +
  geom_boxplot(shape = "circle", fill = "lightgreen", color = "black") +
  theme_minimal() +
  labs(
    title = "Duration Bins"
  )


CleanFlix %>% mutate(region_type = if_else(internation_flag == FALSE,"Domestic","International")) %>% 
                    ggplot(aes(x = `Imdb Score`)) +
                      geom_histogram(bins = 30L, fill = "magenta", color = "black") +
                      theme_minimal() +
                      facet_wrap(vars(region_type)) +
                      ylab("Count")


```



\pagebreak
# Creating Test/Train Data

The next step is to break the data into a Test and Train set with about 50% for both groups.

```{r echo=FALSE, message=FALSE, warning=FALSE}

set.seed(73)

row_count <- nrow(CleanFlix)

## Add a column with random number

ModelFlix <- CleanFlix
rownames(ModelFlix) <- ModelFlix$`Show Id`

ModelFlix <- ModelFlix %>% mutate(
                                  random_seed = runif(row_count,1, 100) / 100,
                                  model_group = if_else(random_seed<= 0.50, "Train","Test"),
                                  Train = model_group == "Train",
                                  Test = model_group == "Test",
                                  ) %>% dplyr::select(model_group,Train, Test, `Imdb Score`, Rating, Duration_bins, internation_flag, 
                                                      Genres, Director, Cast)

sapply(ModelFlix[,2:3], sum)
barplot(sapply(ModelFlix[,2:3], sum) / row_count, main = "Test & Training IMDb")


```

\pagebreak
# Predicting IDMB Scores
## Model Results 1
Looking at Rating, Duration Bins, International Flag, Genres and Director to help build each of the models

```{r echo=FALSE, message=FALSE, warning=FALSE}


train <- ModelFlix %>% filter(model_group=="Train") %>%  dplyr::select(`Imdb Score`, Rating, Duration_bins, internation_flag, Director, Genres)

# Creating a control with cross validation of 3
control <- trainControl(method ='cv',number = 3)

# Metric for comparison will be accuracy for this project
# metric <-  "Accuracy"

# Decision Tree

tree.model <- train(`Imdb Score` ~ ., data = train, method="rpart",  trControl=control,
                    tuneLength = 3)


# Support Vector Machine (SVM)

svm.model <- train(`Imdb Score` ~ ., data = train, method="svmRadial",trControl=control,
                   tuneLength = 3)

# kNN

knn.model <- train(`Imdb Score` ~ ., data = train, method="knn",  trControl=control,
                   tuneLength = 3)

# Random Forest
rf.model <- train(`Imdb Score` ~ ., data = train, method="rf",  trControl=control,
                  tuneLength = 3)


```

\pagebreak
# Interpreting the Results

-   When using cross validation on the Train data set, Random Forest looks to be the best among SVM, knn, and Decesion Tree.
-   The Rsquared is about 2.2 for Random Forest and the Root Mean Square Error is lower the rest of the other models.
-   That being said, overall the Rsquared values are not at a favorable state to accept the model.
-   Additionally, when trying to test the model, Director and Genre are too unique to be used and, therefore, the model errors out.

```{r echo=FALSE}


# summarize accuracy of models
results <- resamples(list(Decesion_Tree=tree.model,
                          knn=knn.model,
                          SVM=svm.model,Random_Forest=rf.model))
summary(results)

dotplot(results)


```


\pagebreak
## Model Results 2
Looking at Rating, Duration Bins, and International Flag to help build the models.

```{r echo=FALSE, message=FALSE, warning=FALSE}


train <- ModelFlix %>% filter(model_group=="Train") %>%  dplyr::select(`Imdb Score`, Rating, Duration_bins, internation_flag)

# Creating a control with cross validation of 3
control <- trainControl(method ='cv',number = 3)

# Metric for comparison will be accuracy for this project
# metric <-  "Accuracy"


# Decision Tree

tree.model <- train(`Imdb Score` ~ ., data = train, method="rpart",  trControl=control,
                    tuneLength = 3)


# Support Vector Machine (SVM)

svm.model <- train(`Imdb Score` ~ ., data = train, method="svmRadial",trControl=control,
                   tuneLength = 3)

# kNN

knn.model <- train(`Imdb Score` ~ ., data = train, method="knn",  trControl=control,
                   tuneLength = 3)

# Random Forest
rf.model <- train(`Imdb Score` ~ ., data = train, method="rf",  trControl=control,
                  tuneLength = 3)


```

\pagebreak
# Interpreting the Results

-   All of the models have an Rsquared just over 0.
-   The RMSE's are also still too high for any of the models to be considered successful.
-   Overall, the model would be hard to accept in applying to real world solutions like predicting and improving the IMDb score based on these attributes.

```{r echo=FALSE}


# summarize accuracy of models
results <- resamples(list(Decesion_Tree=tree.model,
                          knn=knn.model,
                          SVM=svm.model,Random_Forest=rf.model))
summary(results)

dotplot(results)


```



```{r echo=FALSE}

test <- ModelFlix %>% filter(model_group=="Test") %>% dplyr::select(`Imdb Score`, Rating, Duration_bins, internation_flag)

```



\pagebreak
# R Squared Results for Models
## Reviewing SVM Model

-   The computational cost goes up in order to identify the range of RMSE's that could potentially occur by using this model.
-   Said a different way, the cost to cross validate increases substantially when moving from the second cross validation to the third cross validation.


```{r echo=FALSE}

svm.model
plot(svm.model)

svm_rsq <- mean(svm.model$resample$Rsquared)
svm_rmse <- mean(svm.model$resample$RMSE)

Rsquared <- data.frame(svm_rsq)
RMSE <- data.frame(svm_rmse)
  
  
```


\pagebreak
## Reviewing KNN Model

-   Looking at knn for cross validating 3 points, the RMSE goes down as we introduce more number of neighbors.

```{r echo=FALSE}

knn.model
plot(knn.model)

knn_rsq <- mean(knn.model$resample$Rsquared)
knn_rmse <- mean(knn.model$resample$RMSE)

Rsquared <- data.frame(cbind(Rsquared, knn_rsq))
RMSE <- data.frame(cbind(RMSE,knn_rmse))

```

\pagebreak
## Reviewing Decision Tree Model

-   The complexity of the decision tree dramatically goes up during cross validation.
-   From the first to the second point, the increase is only marginal.
-   When going to the third cross validation point, the jump is more than 6 times.

```{r echo=FALSE}

tree.model
plot(tree.model)

tree_rsq <- mean(tree.model$resample$Rsquared)
tree_rmse <- mean(tree.model$resample$RMSE)

Rsquared <- data.frame(cbind(Rsquared, tree_rsq))
RMSE <- data.frame(cbind(RMSE,tree_rmse))

```


\pagebreak
## Reviewing Random Forest

-   This model takes the longest to run and, in order to capture a range of 3 RMSE's points, 10 randomly selected predictors had to be introduced.

```{r echo=FALSE}

rf.model
plot(rf.model)

rf_rsq <- mean(rf.model$resample$Rsquared)
rf_rmse <- mean(rf.model$resample$RMSE)

Rsquared <- data.frame(cbind(Rsquared, rf_rsq))
RMSE <- data.frame(cbind(RMSE,rf_rmse))


```


\pagebreak
## Overall R-Squared and RMSE

Before jumping into predicting IMDb scores, the average Rsquares and RMSE's are as followes:

```{r echo=FALSE}

Rsquared
RMSE


```



\pagebreak
# Pridicting IMDB Scores

-   Something interesting happened here, regardless of how low all of the Rsquares were and how how the RMSE were, knn was able to achieve a RMSE of 0.07 much lower than what the cross validation chose.
-   The hypothesis to why K Nearest Neighbor is able to achieve these results is because most of the data is around an IMDb score of 6.  knn captures similarity by looking at the distance or closeness to each data point.  Meaning that the Rsquared is a great validation method to understand if the input variable can explain the change in the target variable (IMDb Score).
-   Due to the data already being so close to 6, a user would be better off just guessing the score.

```{r echo=FALSE}

# The code above and graphics displays the summary of the model just created in
# the process. It is important to note and find the final model that is selected
# which has the highest accuracy.To do so in random forest model out of 3 fold,
# one needs to check which value of mtry that yielded the most accuracy, in this
# example it was mtry = 39.However as the mtry is increased the accuracy of the
# model seems to go down.

# The random forest algorithm selects the random label and creates the tree of
# it's own. The final model is the one that yields the highest accuracy. Mtry is
# the number associated with that specific tree.

# Actual
prediction <- test %>% dplyr::select(`Imdb Score`)

# Prediction on the test data using decision tree

tree <- predict(tree.model, test)

prediction <- data.frame(cbind(prediction,tree))


# Prediction on the test data using svm
svm <- predict(svm.model, test)

prediction <- data.frame(cbind(prediction, svm))

# Prediction on the test data using knn
knn <- predict(knn.model, test)

prediction <- data.frame(cbind(prediction, knn))



# Prediction on the test data using random forest
random_f <- predict(rf.model, test)

prediction <- data.frame(cbind(prediction, random_f))


head(prediction)


test_results <- RMSE %>% mutate(results = "Train") %>% relocate(results, .before = svm_rmse)

train_results <- prediction %>% summarise(
                          results = "Test",
                          svm_rmse = sqrt(mean((Imdb.Score - svm)^2)),
                          knn_rmse = sqrt(mean(Imdb.Score - knn)^2),
                          tree_rmse = sqrt(mean((Imdb.Score - tree)^2)),
                          rf_rmse = sqrt(mean((Imdb.Score - random_f)^2))
                        )


test_results %>% union_all(train_results) 

```

\pagebreak

# Association Rule Mining

Association rule mining required the data to be transaction type structure.  Meaning that each movie will be treated as a separate transaction with, at most, 3 different genres associated to them.  The goal for using this method is to identify the next movie to watch by simply looking to genre.

-   The most watched type of movie is either Drama or Comedies.
-   After those two genres, the movies step down dramatically from around 350 to 200 titles.

```{r echo=FALSE, message=FALSE, warning=FALSE}

FlixGenre <- CleanFlix %>% dplyr::select(Genres)

FlixGenreRows <- FlixGenre %>% mutate(Genres2 = strsplit(Genres,", ")) %>% unnest(Genres2)

FlixGenre <- separate(FlixGenre, "Genres", paste("Genres", 1:3, sep = "_"), sep = ",", extra = "drop") %>% 
                        mutate(across(contains("Genres"), ~ as.factor(.)))

write_csv(FlixGenre, "FlixGenre.csv")

GenreTransactions <- read.transactions("FlixGenre.csv", format = "basket", sep=",", skip = 1)


arules::inspect(GenreTransactions[1:20])

# Create an item frequency plot for the top 20 items
itemFrequencyPlot(GenreTransactions,topN=20,type="absolute")

```

\pagebreak
The data is now in a transaction format and the next step includes looking at *support* and *confidence*:
-   *support* is an indication of how frequently an item appear in the data
-   *confidence* indicates the number of times the if-then statements are found true.

-   Setting the minimum support to 0.001 pulls as many items into the dataset, but not all items to avoid bringing transactions that did not have as many associations.
-   The minimum confidence is set to 0.8 in order to bring in rules that are significant in making actionable decisions.

Looking to some summary information about the rules illuminates some information about the data:

-   The number of rules generated: 8.
-   The distribution of rules by length: most rules are 3 items long.
-   The summary of quality measures: interesting to see ranges of support, lift, and confidence.
-   The information on the data mined: total data mined and minimum parameters.

```{r echo=FALSE}

# generate rules
# Dan why did you set the support, confidence, and maxlen as these values.
rules <- apriori(GenreTransactions,  parameter = list(supp = 0.001, conf = 0.8))

# Rounding rules to 2 digits
options(digits=2)

# get summary info about all rules
summary(rules)



```



## Exploring Metrics to Evaluate - Confidence

-   The chart below shows a general sense of what the rules look like when sorting by confidence.
-   The most confident predictors are when an individual watches an international or independent film that is either about Sports or LGBTQ has a confidence of 1 and in only one case 0.86. The movies suggested to watch next is a drama.
-   In the case of the movie currently being watched with international and sports as the genre tags, the next movie recommended would be a documentary.
-   If a person watches a horror movie and from an international production country, the next movie suggestion should be a thriller.


```{r echo=FALSE}

# sort the rules to view most relevant first (confidence)
rules <- sort(rules, by="confidence", decreasing=TRUE)
arules::inspect(rules)


```

\pagebreak
# Plotting the Association Rule

Most movies lead back to Dramas, Documentaries, or Thrillers, with a high support for Drama.

```{r echo=FALSE}

plot(rules,method="graph",shading=NA)

```

# Conclusion

In terms of accurately grouping movies as they arrive in the recommender system, clustering and classification both struggled due primarily to the low frequency of words in the description with an average of 14 words per movie. Classification did end up slightly more successful than clustering with about a 54 percent success rate but still did not perform well with movies that had lower parental ratings. Pulling a summary of the movie from other websites with more information about the film might make classification and clustering viable methods for initially grouping the data in the future. Additionally, getting more movies with lower ratings into the system could help the reliability of clustering and classification as movies with a R rating greatly outnumbered the other movies.

Overall, the models were not able to provide value in predicting IMDb score.  More attributes might help in having the precision to to predict a score but simply using Genre, Rating, if the movie was international and Duration Bins, does not help explain the score.  K-Nearest Neighbors appeared to have an incredibly low Root Mean Square error but is overshadowed by the cross validation reducing the ability to adopt the model.

This association rule mining technique proved to be valuable in helping identify the next movie to watch.  Although Drama was the most watched genre, what was interesting is the models technique identified for international sport movies that a Netflix watcher should jump to a Documentary next.  This is outside the norm of what is expected in just normal behavior.  In addition, the lift was substantially higher than the other movies with confidence levels over 0.80.  That suggestion could have a noticeable impact on helping Netflix watchers discover what they might like to watch next.

## References

Makhija, S. (2021, July). Netflix Movies and TV Shows 2021. Kaggle. https://www.kaggle.com/satpreetmakhija/netflix-movies-and-tv-shows-2021

---
title: "Homework 9"
author: "Daniel Caley"
date: "8/28/2021"
output: 
  pdf_document:
    toc: TRUE
---


The homework for week 9 is exercises 1, 5, 6 and 7 on page 234.

```{r setup, include=FALSE}

# install.packages("DescTools")
#install.packages("BaylorEdPsych")
#packageurl <- "https://cran.r-project.org/src/contrib/Archive/BaylorEdPsych/BaylorEdPsych_0.5.tar.gz"
#install.packages(packageurl, repos=NULL, type="source")



#require(devtools)
library(BaylorEdPsych)
library(car)
library(tidyverse)
library(MCMCpack) 

```

\pagebreak
# Question 1: Logistic Regression


The built‐in data sets of R include one called `mtcars`, which stands for Motor Trend cars. Motor Trend was the name of an automotive magazine and this data set contains information on cars from the 1970s. Use `?mtcars` to display help about the data set. The data set includes a dichotomous variable called vs, which is coded as 0 for an engine with cylinders in a `v‐shape` and 1 for so called `straight` engines. Use logistic regression to predict vs, using two metric variables in the data set, gear (number of forward gears) and hp (horsepower). Interpret the resulting null hypothesis significance tests.


## Interpretting the Results

-   Horsepower  with a Z value of -2.455 is significant with a P-value of 0.0141.
-   Gear does not have a significant P-value
-   The Chi Square test shows a difference of 26.4814 at a significant P-value of  0.00000026.
Gear does not a significant P-Value
-   The graph illustrate that for each unit change in the value of X, odds that Y=1 is the correct prediction increases by 3.78:1 for gears and 9.123:1 for hp.
-   Said differently as hp increases by 1 unit, mpg deceases by 92 percent for every unit.

```{r}

?mtcars

MyMtcars <- mtcars

LogitCars <- glm(vs ~ gear + hp, MyMtcars, family = binomial())
summary(LogitCars)

anova(LogitCars, test = "Chisq")


CarsCoef <-  exp(coef(LogitCars))
CarsCoef

logOdds <- seq(from=-2, to = 2, length.out=100)

plot(logOdds, exp(logOdds))
abline(v = log(CarsCoef[2]))
abline(v = log(CarsCoef[3]))


```

\pagebreak
# Question 5

As noted in the chapter, the `BaylorEdPsych` add‐in package contains a procedure for generating pseudo‐R‐squared values from the output of the `glm()` procedure. Use the results of Exercise 1 to generate, report, and interpret a Nagelkerke pseudo‐R‐squared value.
```{r}


PseudoR2(LogitCars)


```

## Interpreting the Results

The Suedo R-Sqaured shows a value of 0.7789.  Said differently 78% of the predictor variables (HP and Gear) can explain the engine shape.



\pagebreak
# Question 6

Continue the analysis of the Chile data set described in this chapter. The data set is in the `car` package, so you will have to `install.packages()` and `library()` that package first, and then use the `data(Chile)` command to get access to the data set. Pay close attention to the transformations needed to isolate cases with the Yes and No votes as shown in this chapter. Add a new predictor, `statusquo`, into the model and remove the income variable. Your new model specification should be `vote ~ age + statusquo`. The `statusquo` variable is a rating that each respondent gave indicating whether they preferred change or maintaining the status quo. Conduct general linear model and Bayesian analysis on this model and report and interpret all relevant results. Compare the AIC from this model to the AIC from the model that was developed in the chapter (using income and age as predictors).

```{r}

MyChile <- Chile %>% 
            filter(vote %in% c("Y","N")) %>%
            mutate(
                   vote =  factor(vote,levels=c("N","Y")),
                   vote = as.numeric(vote) - 1
                   )

MyChile <- MyChile[complete.cases(MyChile),] # Get rid of missing

LogitChile <- glm(vote ~ age + statusquo, MyChile,family = binomial())
summary(LogitChile)
anova(LogitChile, test= "Chisq")
PseudoR2(LogitChile)


ChilCoef <-  exp(coef(LogitChile))
ChilCoef

logOdds <- seq(from=-2, to = 2, length.out=100)

plot(logOdds, exp(logOdds))
abline(v = log(ChilCoef[2]))
abline(v = log(ChilCoef[3]))

# Bayes Piece
set.seed(271) # Control randomization
bayesLogitOut <- MCMClogit(formula = vote ~ age + statusquo, data = MyChile)
summary(bayesLogitOut) # Summarize the results

# Age
ageLogOdds <- as.matrix(bayesLogitOut[,"age"])
ageOdds <- apply(ageLogOdds,1,exp) # Transform with exp()
mean(ageOdds) # The point estimate for age in plain odds
quantile(ageOdds,c(0.025)) # Lower bound of HDI
quantile(ageOdds,c(0.975)) # Upper bound of HDI



hist(ageOdds)
abline(v=quantile(ageOdds,c(0.025)), col='salmon')
abline(v=quantile(ageOdds,c(0.975)), col='salmon')


actualVote <- MyChile$vote
predictedVote <- round(predict(LogitChile, type='response')) # round() splits probabilities at 0.5
ChiliConfus <-  table(predictedVote, actualVote)
ChiliConfus

print("error rate:")


yvote <- ChiliConfus[2,1]
agevote <- ChiliConfus[1,2]

(yvote+agevote)/sum(ChiliConfus)

```

## Interpretting the results

-   The Suedo R-squared is 68% indicating not a strong relationship between predicting if the Chilean plebiscite would be voted on.
-   The p-values for age is 0.097 which is not under the 0.05 alpha which we would want.
-   The p-value for status quo is under the 0.05 at 2-e16.


When interpretting the Bayes Theorom we notice the following.

We examined data from the 1988 Chilean plebiscite, to see if the age and statuesque of a voter could predict whether an individual would vote in favor of keeping Augusto Pinochet in office. We conducted a Bayesian logistic analysis, using age and statuesque to predict votes.  The Highest Density Interval of age overlap with zero. When converted to regular odds, the mean value of the posterior distribution for age was 1.01 to 1, suggesting that for every additional year of age, an individual was about 1% more likely to vote to keep Pinochet. In addition with the p-value being so low for the Logit Regression model for age is not a significant determinant of voting behavior.  Status Quo appears to be the better predictor. 
The confusion matrix showed that the overall error rate was 8% indicating that the logistic model for age was good at predicting votes.  Though the HDI does overlap for 0 barely for age we fail to reject the null hypothesis.


\pagebreak
# Question 7

Bonus R code question: Develop your own custom function that will take the posterior distribution of a coefficient from the output object from an `MCMClogit()` analysis and automatically create a histogram of the posterior distributions of the coefficient in terms of regular odds (instead of log‐odds). Make sure to mark vertical lines on the histogram indicating the boundaries of the 95% HDI.

```{r}

# Status Quo Funciton

PostDistroHist <- function(x,y){
  stutusQuoLogOdds <- as.matrix(x[,y])
  stutusQuoLogOdds <- apply(stutusQuoLogOdds,1,exp) # Transform with exp()
  mean(stutusQuoLogOdds) # The point estimate for age in plain odds
  quantile(stutusQuoLogOdds,c(0.025)) # Lower bound of HDI
  quantile(stutusQuoLogOdds,c(0.975)) # Upper bound of HDI
  
  
  
  hist(stutusQuoLogOdds)
  abline(v=quantile(stutusQuoLogOdds,c(0.025)), col='salmon')
  abline(v=quantile(stutusQuoLogOdds,c(0.975)), col='salmon')

}

PostDistroHist(bayesLogitOut, "statusquo")



```





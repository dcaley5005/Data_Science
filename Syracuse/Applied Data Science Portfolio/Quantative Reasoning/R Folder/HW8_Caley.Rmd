---
title: "Homework 8"
author: "Daniel Caley"
date: "8/28/2021"
output: 
  pdf_document:
    toc: TRUE
---

The homework for week 8 is exercises 1-8 on pages 181-182.

```{r setup, include=FALSE}

library(BayesFactor)
library(corrplot)

```


\pagebreak
# Question 1: MyCars DataFrame

The data sets package in R contains a small data set called mtcars that contains n = 32 observations of the characteristics of different automobiles. Create a new data frame from part of this data set using this command: `myCars <‐ data.frame(mtcars[,1:6])`.

```{r}

myCars <- data.frame(mtcars[,1:6])
myCars[1:5,1:5]

```

\pagebreak
# Question 2: MyCars Correlation
Create and interpret a bivariate correlation matrix using `cor(myCars)` keeping in mind the idea that you will be trying to predict the mpg variable. Which other variable might be the single best predictor of mpg?

## Interpreting the Results

Some good predictors might by cyl, disp, and wt.  With all of these independent variable below -0.80.  We either want a high negative correlation or a high positive correlation.  HP might also be a good variable, but does not have the highest correlation.  We will run verify when running in the linear regression model.

```{r}


myCars_cor <- cor(myCars)

corrplot(myCars_cor, method = "number")

```



\pagebreak
# Question 3 MyCar Linear Regression

Run a multiple regression analysis on the `myCars` data with `lm()`, using mpg as the dependent variable and wt (weight) and hp (horsepower) as the predictors. Make sure to say whether or not the overall R‐squared was significant. If it was significant, report the value and say in your own words whether it seems like a strong result or not. Review the significance tests on the coefficients (B‐weights). For each one that was significant, report its value and say in your own words whether it seems like a strong result or not.

## Interpreting the Results

-   The p-value of the F Statistics is well below 0.05 at .0001
-   The Adjusted R-squared is at 0.8148 which is very strong.
-   The Adjusted R-squared is saying that 81.5% of the dependant variables can be explained by the independent variables.
-   The individual p-values are alos very strong with being well below 0.05 at 0.001.

```{r}

MyCarsLm <- lm(mpg ~ wt + hp ,myCars)
summary(MyCarsLm)


```


\pagebreak
# Question 4
Using the results of the analysis from Exercise 2, construct a prediction equation for mpg using all three of the coefficients from the analysis (the intercept along with the two B‐weights). Pretend that an automobile designer has asked you to predict the mpg for a car with 110 horsepower and a weight of 3 tons. Show your calculation and the resulting value of mpg.

## Building an Equation

the equation for the linear regression model would be the following.

mpg = -3.87783(wt) + -0.03177(hp) + 37.22727

22.09908 = -3.87783(3) + -0.03177(110) + 37.22727

MPG for this type of car would be 22.09908.


```{r}

hp <- 110
wt <- 3

-3.87783 * wt + -0.03177* hp + 37.22727

```


\pagebreak
# Question 5
Run a multiple regression analysis on the `myCars` data with `lmBF()`, using mpg as the dependent variable and wt (weight) and hp (horsepower) as the predictors. Interpret the resulting Bayes factor in terms of the odds in favor of the alternative hypothesis. If you did Exercise 2, do these results strengthen or weaken your conclusions?

## Interpretting the Results

The odds are extremely high at 788547604:1 in which Bayes Factor overwhelmingly favors a model that includes the three predictors.

```{r}



MyCarsBFLm <- lmBF(mpg ~ wt + hp ,data = myCars, )
MyCarsBFLm


```


# Question 6
Run `lmBF()` with the same model as for Exercise 4, but with the options posterior=TRUE and iterations=10000. Interpret the resulting information about the coefficients.

## Interpretting the results

We tested a model of Car specifications that used two variables to predict MPG: Weight and Horsepower, A Bayesian analysis of this model showed a mean posterior estimate for R-squared of 0.794, with the highest density interval ranging from roughly 0.19 to 0.21. The traditional analysis confirmed this result with a slightly more optimistic R-squared of 0.815. The F-test on this value was F(2, 29)=69.21.0, p<.001, so we reject the null hypothesis that R-squared was equal to zero. Both predictors were also significant with B-weights less than 0.001 for weight and horsepower. The Bayes factor of 788547604 was strongly in favor of the two predictor model (in comparison with an intercept-only model).


```{r}


MyCarsBFmcmc <- lmBF(mpg ~ wt + hp ,data = myCars, posterior=TRUE, iterations=100000)
summary(MyCarsBFmcmc)
rsqList <- 1 - (MyCarsBFmcmc[,"sig2"] / var(myCars$mpg))
mean(rsqList)
quantile(rsqList,c(0.025))
quantile(rsqList,c(0.975))


```

\pagebreak
# Question 7
Run install.packages() and library() for the `car` package. The `car` package is "companion to applied regression" rather than more data about automobiles. Read the help file for the `vif()` procedure and then look up more information online about how to interpret the results. Then write down in your own words a “rule of thumb” for interpreting `vif.`

## Interperting vif

The definition from the help function is as followed, calculates variance-inflation and generalized variance-inflation factors (VIFs and GVIFs) for linear, generalized linear, and other regression models.

In simpler terms vif helps in solving for and identify multicollinearity.  That is there might be redundancies between predictor variables in which will hurt the model.  Vif computes a score called the variance inflation factor in which measures how much the variance of a regression coefficient is inflated due to multicollinearity in the model.

A value that exceed 5 or 10 indicates a problematic amount of collinearity.



```{r}

library(car)


```


\pagebreak
# Question 8
Run `vif()` on the results of the model from Exercise 2. Interpret the results. Then run a model that predicts mpg from all five of the predictors in `myCars`. Run `vif()` on those results and interpret what you find.

Interpreting the results.

-   The first model from exercise 2 shows that there is no multicollinearity where values are less than 5 and 10.
-   The second model from looking at all independent variables show that cycl, wt, disp all have multicollinarity with being above 5 and 10.
-   We either need to drop variables or due some sort of mutation in order to reduce multicolinarity.

```{r}

vif(MyCarsLm)

FivePredictor_Model <- lm(mpg ~., myCars)

summary(FivePredictor_Model)

vif(FivePredictor_Model)



```





